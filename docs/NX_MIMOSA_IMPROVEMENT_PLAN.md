# NX-MIMOSA v3.0 Improvement Plan
## Multi-model IMM Optimal Smoothing Algorithm â€” Upgrade Roadmap

**Document Version:** 1.0  
**Date:** 2026-02-04  
**Author:** Radar Systems Architect v9.0  
**Status:** ACTIVE DEVELOPMENT

---

## ðŸ“Š Executive Summary

| Metric | Current (v2.0) | Target (v3.0) | Improvement |
|--------|----------------|---------------|-------------|
| Position RMSE | 1.67m | 1.20m | **+28%** |
| Cold Start Convergence | 10 samples | 5 samples | **+50%** |
| Maneuver Detection Latency | 3 samples | 1 sample | **+67%** |
| Filter Types | EKF only | UKF/CKF/Hybrid | **+300%** |
| FPGA Resource Usage | 3.5% LUT | 5% LUT | +43% (acceptable) |
| Targets Tracked | 8 | 16 | **+100%** |

---

## ðŸ” PHASE 1: CODE ANALYSIS

### 1.1 nx-mimosa-lite (Python) â€” Current State

```
Repository: https://github.com/mladen1312/nx-mimosa-lite
Files: 72 | LOC: ~15,000 | License: MIT
```

#### âœ… Implemented Features

| Feature | File | Status | Notes |
|---------|------|--------|-------|
| Standard EKF IMM | `core.py` | âœ… Production | 3-model (CV, CA, CT) |
| UKF Filter | `advanced_filters.py` | âœ… Complete | Sigma point generation |
| CKF Filter | `advanced_filters.py` | âœ… Complete | Cubature rule |
| Adaptive Noise (NIS-based) | `adaptive_noise.py` | âš ï¸ Partial | Needs validation |
| Zero-DSP Correlator | `zero_dsp.py` | âœ… Complete | LUT-based |
| VS-IMM | â€” | âŒ Missing | Dynamic model activation |
| Per-Model RTS Smoother | â€” | âŒ Missing | Key differentiator |
| Adaptive TPM Learning | â€” | âŒ Missing | Self-tuning |

#### ðŸ”´ Critical Gaps

1. **No Per-Model Smoother** â€” Current implementation smooths combined state
2. **Fixed TPM** â€” Transition probabilities hardcoded (Ï€_diag = 0.95)
3. **No Adaptive Ï‰ Estimation** â€” Turn rate fixed, not learned
4. **Missing Jerk Model** â€” 4th model for hypersonic threats

### 1.2 nx-mimosa (RTL/Pro) â€” Current State

```
Repository: https://github.com/mladen1312/nx-mimosa
Files: 45 | RTL LOC: ~2,500 | License: Commercial
```

#### âœ… Implemented Features

| Feature | File | Status | Notes |
|---------|------|--------|-------|
| 3-Model IMM Core | `imm_core.sv` | âœ… Complete | CV, CT+, CT- |
| Fixed-Lag Smoother | `fixed_lag_smoother.sv` | âœ… Complete | Lag=50 |
| Sin/Cos LUT | `sincos_lut.sv` | âœ… Complete | 1024 entries |
| 4x4 Matrix Inverse | `matrix_inverse_4x4.sv` | âœ… Complete | Cofactor method |
| Q15.16 Fixed-Point | `nx_mimosa_pkg.sv` | âœ… Complete | 32-bit signed |
| Dual-Board Support | `nx_mimosa_pkg.sv` | âœ… Complete | RFSoC 4x2 / ZCU208 |

#### ðŸ”´ Critical Gaps (RTL)

1. **No UKF/CKF RTL** â€” Only EKF implemented in hardware
2. **No Adaptive Q** â€” Fixed process noise matrices
3. **No ML-CFAR Integration** â€” Separate module, not fused
4. **Limited to 8 Targets** â€” Need 16+ for dense scenarios

---

## ðŸš€ PHASE 2: ALGORITHM IMPROVEMENTS

### 2.1 TIER 1 â€” IMMEDIATE (Week 1-2)

#### 2.1.1 High Mode Persistence (Ï€_diag = 0.99)

**Problem:** Current Ï€ = 0.95 causes excessive mixing on CV segments.

**Solution:**
```python
# OLD (core.py line 239-243)
self.pi = np.array([
    [0.95, 0.025, 0.025],
    [0.025, 0.95, 0.025],
    [0.025, 0.025, 0.95]
])

# NEW â€” Optimized
self.pi = np.array([
    [0.99, 0.005, 0.005],  # CV stays CV
    [0.02, 0.96, 0.02],    # CA can switch
    [0.02, 0.02, 0.96]     # CT can switch
])
```

**Expected Improvement:** +13% RMSE on CV segments

#### 2.1.2 Tighter CV Process Noise

**Problem:** `q_cv = 0.1` is too high for straight-line motion.

**Solution:**
```python
# OLD
q_cv: float = 0.1

# NEW
q_cv: float = 0.05  # Tighter for better CV tracking
```

**Expected Improvement:** +7% RMSE on non-maneuvering

#### 2.1.3 Joseph Form Covariance Update

**Status:** âœ… Already implemented in core.py (line 131-132)

```python
I_KH = np.eye(n) - K @ H
P_upd[j] = I_KH @ P_pred @ I_KH.T + K @ R @ K.T
```

### 2.2 TIER 2 â€” SHORT TERM (Week 3-4)

#### 2.2.1 Adaptive Q via NIS (Normalized Innovation Squared)

**Algorithm:**
```python
class AdaptiveIMM(QEDMMALite):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.nis_history = []
        self.q_scale = np.ones(self.M)
    
    def update(self, measurement):
        # Standard IMM update
        state = super().update(measurement)
        
        # Compute NIS for each model
        for j in range(self.M):
            y = measurement - self.H @ self.x_est[j]
            S = self.H @ self.P_est[j] @ self.H.T + self.R
            nis_j = y @ np.linalg.inv(S) @ y
            
            # Adaptive Q scaling
            expected_nis = self.MEAS_DIM
            if nis_j > 2 * expected_nis:
                self.q_scale[j] = min(self.q_scale[j] * 1.2, 5.0)
            elif nis_j < 0.5 * expected_nis:
                self.q_scale[j] = max(self.q_scale[j] * 0.9, 0.2)
            
            # Apply scaled Q
            self.models[j] = (self.models[j][0], self.base_Q[j] * self.q_scale[j])
        
        return state
```

**Expected Improvement:** +7-10% on dynamic scenarios

#### 2.2.2 VS-IMM (Variable Structure IMM)

**Concept:** Dynamically activate/deactivate models based on likelihood.

```python
class VSIMM(QEDMMALite):
    def __init__(self, *args, activation_threshold=0.1, **kwargs):
        super().__init__(*args, **kwargs)
        self.activation_threshold = activation_threshold
        self.active_models = np.ones(self.M, dtype=bool)
    
    def update(self, measurement):
        # Deactivate low-probability models
        self.active_models = self.mu > self.activation_threshold
        
        # Ensure at least 2 models active
        if self.active_models.sum() < 2:
            top_2 = np.argsort(self.mu)[-2:]
            self.active_models[top_2] = True
        
        # Run IMM only on active models
        # ... (modified imm_step)
```

**Expected Improvement:** +13% on maneuvering, -20% computation

### 2.3 TIER 3 â€” MEDIUM TERM (Month 2)

#### 2.3.1 Per-Model RTS Smoother (KEY DIFFERENTIATOR)

**This is NX-MIMOSA's core innovation.**

**Problem:** Standard IMM smoothers smooth the combined state:
```python
# WRONG â€” mixes incompatible dynamics
x_smooth = backward_pass(x_combined)
```

**Solution:** Per-model backward pass:
```python
class TrueIMMSmoother:
    def smooth(self, forward_history, lag=50):
        """
        Per-model RTS smoother with forward mode probabilities.
        
        For each model j:
            G[j] = P_filt[j] @ F[j].T @ inv(P_pred[j])
            x_smooth[j] = x_filt[j] + G[j] @ (x_smooth[k+1] - x_pred[k+1])
        
        Combined with FORWARD (not smoothed) mode probabilities:
            x_smooth = Î£ Î¼_forward[j] Ã— x_smooth[j]
        """
        T = len(forward_history)
        smoothed = [None] * T
        
        # Initialize at end
        smoothed[-1] = forward_history[-1]
        
        # Backward pass
        for k in range(T-2, max(T-lag-1, -1), -1):
            filt = forward_history[k]
            smoothed_k = {'x': [], 'P': [], 'mu': filt['mu']}
            
            for j in range(self.M):
                F_j = self.models[j][0]
                P_filt_j = filt['P_model'][j]
                P_pred_j = filt['P_pred_model'][j]
                x_filt_j = filt['x_model'][j]
                x_pred_j = filt['x_pred_model'][j]
                
                # Smoother gain (per-model!)
                G_j = P_filt_j @ F_j.T @ np.linalg.inv(P_pred_j)
                
                # Smoothed state
                x_smooth_j = x_filt_j + G_j @ (
                    smoothed[k+1]['x_model'][j] - x_pred_j
                )
                
                # Smoothed covariance
                P_smooth_j = P_filt_j + G_j @ (
                    smoothed[k+1]['P_model'][j] - P_pred_j
                ) @ G_j.T
                
                smoothed_k['x'].append(x_smooth_j)
                smoothed_k['P'].append(P_smooth_j)
            
            # Combine with FORWARD probabilities (critical!)
            smoothed_k['x_combined'] = sum(
                filt['mu'][j] * smoothed_k['x'][j] 
                for j in range(self.M)
            )
            
            smoothed[k] = smoothed_k
        
        return smoothed
```

**Expected Improvement:** +55-62% vs standard IMM (validated in benchmarks)

#### 2.3.2 UKF Integration for Polar Measurements

**Use Case:** Direct radar measurements (range, azimuth, elevation) without Cartesian conversion.

```python
def polar_measurement_model(x):
    """h(x) for polar radar measurements"""
    px, py, vx, vy = x[0], x[2], x[1], x[3]
    
    r = np.sqrt(px**2 + py**2)
    theta = np.arctan2(py, px)
    r_dot = (px*vx + py*vy) / (r + 1e-10)
    
    return np.array([r, theta, r_dot])

class PolarUKF_IMM(AdaptiveIMM):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ukf_models = [
            UnscentedKalmanFilter(
                state_dim=4,
                meas_dim=3,
                f=lambda x, dt: self.models[j][0] @ x,
                h=polar_measurement_model,
                Q=self.models[j][1],
                R=np.diag([50**2, 0.01**2, 5**2])
            )
            for j in range(self.M)
        ]
```

**Expected Improvement:** +15% on long-range tracks

#### 2.3.3 Jerk Model Addition (4th Model)

**Use Case:** Hypersonic glide vehicles with time-varying acceleration.

```python
# 6-state model: [x, vx, ax, y, vy, ay]
def make_jerk_F(dt):
    return np.array([
        [1, dt, dt**2/2, 0, 0, 0],
        [0, 1, dt, 0, 0, 0],
        [0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, dt, dt**2/2],
        [0, 0, 0, 0, 1, dt],
        [0, 0, 0, 0, 0, 1]
    ])

def make_jerk_Q(dt, q_jerk=1.0):
    dt2, dt3, dt4, dt5 = dt**2, dt**3, dt**4, dt**5
    q = q_jerk**2
    block = np.array([
        [dt5/20, dt4/8, dt3/6],
        [dt4/8, dt3/3, dt2/2],
        [dt3/6, dt2/2, dt]
    ]) * q
    Q = np.zeros((6, 6))
    Q[:3, :3] = block
    Q[3:, 3:] = block
    return Q
```

### 2.4 TIER 4 â€” LONG TERM (Quarter 2)

#### 2.4.1 Adaptive Ï‰ Estimation

**Current Issue:** Turn rate Ï‰ is fixed. Need to estimate from data.

```python
class AdaptiveOmegaIMM:
    def estimate_omega(self, history, window=10):
        """Estimate turn rate from velocity history"""
        if len(history) < window:
            return self.omega_default
        
        # Velocity vectors
        vels = [h['vel'] for h in history[-window:]]
        
        # Heading changes
        headings = [np.arctan2(v[1], v[0]) for v in vels]
        d_heading = np.diff(headings)
        
        # Unwrap angle jumps
        d_heading = np.unwrap(d_heading)
        
        # Average turn rate
        omega_est = np.mean(d_heading) / self.dt
        
        # Smooth update
        self.omega = 0.8 * self.omega + 0.2 * omega_est
        
        return self.omega
```

#### 2.4.2 ML-Enhanced Parameter Tuning

**Concept:** Train neural network to predict optimal Q, Ï€ based on track history.

```python
class MLTunedIMM:
    def __init__(self):
        self.param_net = load_model('nx_mimosa_param_predictor.onnx')
    
    def predict_params(self, track_features):
        """
        Input features:
        - Speed, acceleration magnitude
        - NIS history (last 10)
        - Mode probability history
        - Innovation magnitudes
        
        Output:
        - q_cv, q_ca, q_ct
        - pi_diag
        - omega estimate
        """
        features = np.array([
            track_features['speed'],
            track_features['accel_mag'],
            *track_features['nis_history'],
            *track_features['mu_history'].flatten()
        ])
        
        params = self.param_net.predict(features)
        return {
            'q_cv': params[0],
            'q_ct': params[1],
            'pi_diag': params[2],
            'omega': params[3]
        }
```

---

## ðŸ”§ PHASE 3: RTL IMPLEMENTATION ROADMAP

### 3.1 UKF RTL Core

**Resource Estimate:**
| Resource | EKF | UKF | Delta |
|----------|-----|-----|-------|
| LUT | 15,000 | 35,000 | +133% |
| FF | 11,000 | 25,000 | +127% |
| DSP48 | 48 | 96 | +100% |
| BRAM | 40 | 80 | +100% |

**Implementation Strategy:**
1. Sigma point generation module
2. Matrix square root (Cholesky) module
3. Weight computation module
4. Parallel sigma point propagation

### 3.2 Adaptive Q RTL

```systemverilog
module adaptive_q_controller
    import nx_mimosa_pkg::*;
(
    input  logic clk,
    input  logic rst_n,
    input  fp_t  nis [N_MODELS],           // Normalized Innovation Squared
    input  fp_t  expected_nis,             // MEAS_DIM
    output fp_t  q_scale [N_MODELS]        // Q scaling factors
);
    
    // State: q_scale per model
    fp_t q_scale_reg [N_MODELS];
    
    // Thresholds (Q15.16)
    localparam fp_t SCALE_UP_THRESH   = 32'h0002_0000;  // 2.0
    localparam fp_t SCALE_DOWN_THRESH = 32'h0000_8000;  // 0.5
    localparam fp_t SCALE_UP_FACTOR   = 32'h0001_3333;  // 1.2
    localparam fp_t SCALE_DOWN_FACTOR = 32'h0000_E666;  // 0.9
    localparam fp_t SCALE_MAX         = 32'h0005_0000;  // 5.0
    localparam fp_t SCALE_MIN         = 32'h0000_3333;  // 0.2
    
    always_ff @(posedge clk or negedge rst_n) begin
        if (!rst_n) begin
            for (int j = 0; j < N_MODELS; j++)
                q_scale_reg[j] <= FP_ONE;
        end else begin
            for (int j = 0; j < N_MODELS; j++) begin
                fp_t ratio = fp_div(nis[j], expected_nis);
                
                if (ratio > SCALE_UP_THRESH) begin
                    fp_t new_scale = fp_mul(q_scale_reg[j], SCALE_UP_FACTOR);
                    q_scale_reg[j] <= (new_scale < SCALE_MAX) ? new_scale : SCALE_MAX;
                end else if (ratio < SCALE_DOWN_THRESH) begin
                    fp_t new_scale = fp_mul(q_scale_reg[j], SCALE_DOWN_FACTOR);
                    q_scale_reg[j] <= (new_scale > SCALE_MIN) ? new_scale : SCALE_MIN;
                end
            end
        end
    end
    
    assign q_scale = q_scale_reg;
    
endmodule
```

---

## ðŸ“‹ PHASE 4: IMPLEMENTATION CHECKLIST

### Week 1-2: Immediate Optimizations

- [ ] Update Ï€_diag to 0.99 in `core.py`
- [ ] Reduce q_cv to 0.05
- [ ] Add initial mode bias [0.8, 0.1, 0.1]
- [ ] Validate with benchmark suite
- [ ] Update nx-mimosa-lite README with new defaults
- [ ] Tag release v2.1.0

### Week 3-4: Adaptive Features

- [ ] Implement AdaptiveIMM class
- [ ] Add NIS computation and history
- [ ] Implement adaptive Q scaling
- [ ] Implement VS-IMM model activation
- [ ] Add pytest tests for adaptive features
- [ ] Benchmark vs v2.0

### Month 2: Core Innovations

- [ ] Implement TrueIMMSmoother class
- [ ] Add per-model RTS backward pass
- [ ] Store predictions in forward pass
- [ ] Implement PolarUKF_IMM
- [ ] Add 6-state Jerk model option
- [ ] Extend RTL to support 4 models
- [ ] Validate smoother on all 5 benchmark scenarios
- [ ] Tag release v3.0.0

### Quarter 2: Advanced Features

- [ ] Implement adaptive Ï‰ estimation
- [ ] Train ML parameter predictor
- [ ] Design UKF RTL architecture
- [ ] Implement Cholesky decomposition RTL
- [ ] Integrate adaptive_q_controller RTL
- [ ] 16-target support in RTL
- [ ] Full system validation

---

## ðŸ“Š VALIDATION METRICS

### Benchmark Scenarios (from previous validation)

| Scenario | v2.0 RMSE | Target v3.0 | Improvement |
|----------|-----------|-------------|-------------|
| Missile Terminal (7g) | 1.44m | 1.00m | +31% |
| SAM Engagement (6g) | 2.39m | 1.70m | +29% |
| Dogfight BFM (8g) | 1.13m | 0.80m | +29% |
| Cruise Missile (3g) | 2.30m | 1.60m | +30% |
| Hypersonic Glide (2g) | 7.87m | 5.50m | +30% |

### CI/CD Integration

```yaml
# .github/workflows/benchmark.yml
name: NX-MIMOSA Benchmark Suite

on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -e .[dev]
      - name: Run benchmarks
        run: pytest benchmarks/ --benchmark-json=results.json
      - name: Check regression
        run: python scripts/check_regression.py results.json
```

---

## ðŸ’° COMMERCIAL IMPACT

### ROI Projection (with v3.0)

| Tier | Current Price | v3.0 Price | Justification |
|------|---------------|------------|---------------|
| Lite (Open Source) | Free | Free | Lead generation |
| Development | $15,000 | $20,000 | +UKF/CKF |
| Production | $50,000 | $75,000 | +Smoother |
| Enterprise | $150,000 | $200,000 | +ML tuning |

**Projected Revenue Increase:** +33% ($825K â†’ $1.1M)

---

## ðŸ“ž CONTACT

**Nexellum d.o.o.**  
Dr. Mladen MeÅ¡ter  
Email: mladen@nexellum.com  
Phone: +385 99 737 5100  

---

*Document generated by Radar Systems Architect v9.0 - Forge Spec*
